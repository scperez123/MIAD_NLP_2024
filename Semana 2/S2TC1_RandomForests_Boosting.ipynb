{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/images/banner_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller: Construcción e implementación de modelos Bagging, Random Forest y XGBoost\n",
    "\n",
    "En este taller podrán poner en práctica sus conocimientos sobre la construcción e implementación de modelos de Bagging, Random Forest y XGBoost. El taller está constituido por 8 puntos, en los cuales deberan seguir las intrucciones de cada numeral para su desarrollo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos predicción precio de automóviles\n",
    "\n",
    "En este taller se usará el conjunto de datos de Car Listings de Kaggle donde cada observación representa el precio de un automóvil teniendo en cuenta distintas variables como año, marca, modelo, entre otras. El objetivo es predecir el precio del automóvil. Para más detalles puede visitar el siguiente enlace: [datos](https://www.kaggle.com/jpayne/852k-used-car-listings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Year</th>\n",
       "      <th>Mileage</th>\n",
       "      <th>M_Camry</th>\n",
       "      <th>M_Camry4dr</th>\n",
       "      <th>M_CamryBase</th>\n",
       "      <th>M_CamryL</th>\n",
       "      <th>M_CamryLE</th>\n",
       "      <th>M_CamrySE</th>\n",
       "      <th>M_CamryXLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>21995</td>\n",
       "      <td>2014</td>\n",
       "      <td>6480</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13995</td>\n",
       "      <td>2014</td>\n",
       "      <td>39972</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>17941</td>\n",
       "      <td>2016</td>\n",
       "      <td>18989</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>12493</td>\n",
       "      <td>2014</td>\n",
       "      <td>51330</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>7994</td>\n",
       "      <td>2007</td>\n",
       "      <td>116065</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Price  Year  Mileage  M_Camry  M_Camry4dr  M_CamryBase  M_CamryL  \\\n",
       "7    21995  2014     6480    False       False        False      True   \n",
       "11   13995  2014    39972    False       False        False     False   \n",
       "167  17941  2016    18989    False       False        False     False   \n",
       "225  12493  2014    51330    False       False        False      True   \n",
       "270   7994  2007   116065    False        True        False     False   \n",
       "\n",
       "     M_CamryLE  M_CamrySE  M_CamryXLE  \n",
       "7        False      False       False  \n",
       "11        True      False       False  \n",
       "167      False       True       False  \n",
       "225      False      False       False  \n",
       "270      False      False       False  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importación de librerías\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "# Lectura de la información de archivo .csv\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/datasets/dataTrain_carListings.zip')\n",
    "\n",
    "# Preprocesamiento de datos para el taller\n",
    "data = data.loc[data['Model'].str.contains('Camry')].drop(['Make', 'State'], axis=1)\n",
    "data = data.join(pd.get_dummies(data['Model'], prefix='M'))\n",
    "data = data.drop(['Model'], axis=1)\n",
    "\n",
    "# Visualización dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de variables predictoras (X) y variable de interés (y)\n",
    "y = data['Price']\n",
    "X = data.drop(['Price'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de datos en set de entrenamiento y test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 1 - Árbol de decisión manual\n",
    "\n",
    "En la celda 1 creen un árbol de decisión **manualmente**  que considere los set de entrenamiento y test definidos anteriormente y presenten el RMSE y MAE del modelo en el set de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1687.5800505908155\n",
      "MAE: 1218.8491606426123\n"
     ]
    }
   ],
   "source": [
    "# Celda 1\n",
    "# Importación de librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Definición de la clase para los nodos del árbol de decisión\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "# Función para dividir el conjunto de datos\n",
    "def split_dataset(X, y, feature_index, threshold):\n",
    "    left_indices = X[:, feature_index] <= threshold\n",
    "    right_indices = X[:, feature_index] > threshold\n",
    "    return X[left_indices], X[right_indices], y[left_indices], y[right_indices]\n",
    "\n",
    "# Función para calcular la varianza\n",
    "def variance(y):\n",
    "    return np.var(y)\n",
    "\n",
    "# Función para calcular la reducción de varianza\n",
    "def variance_reduction(y, y_left, y_right):\n",
    "    weight_l = len(y_left) / len(y)\n",
    "    weight_r = len(y_right) / len(y)\n",
    "    reduction = variance(y) - (weight_l * variance(y_left) + weight_r * variance(y_right))\n",
    "    return reduction\n",
    "\n",
    "# Función para obtener la mejor división\n",
    "def get_best_split(X, y):\n",
    "    best_feature, best_threshold, best_reduction = None, None, -float('inf')\n",
    "    for feature_index in range(X.shape[1]):\n",
    "        thresholds = np.unique(X[:, feature_index])\n",
    "        for threshold in thresholds:\n",
    "            X_left, X_right, y_left, y_right = split_dataset(X, y, feature_index, threshold)\n",
    "            if len(y_left) > 0 and len(y_right) > 0:\n",
    "                reduction = variance_reduction(y, y_left, y_right)\n",
    "                if reduction > best_reduction:\n",
    "                    best_feature, best_threshold, best_reduction = feature_index, threshold, reduction\n",
    "                    best_left, best_right, y_left_best, y_right_best = X_left, X_right, y_left, y_right\n",
    "    return best_feature, best_threshold, best_left, best_right, y_left_best, y_right_best\n",
    "\n",
    "# Función para construir el árbol\n",
    "def build_tree(X, y, max_depth, min_size):\n",
    "    if len(y) <= min_size or max_depth == 0:\n",
    "        return DecisionTreeNode(value=np.mean(y))\n",
    "    feature, threshold, X_left, X_right, y_left, y_right = get_best_split(X, y)\n",
    "    if feature is None:\n",
    "        return DecisionTreeNode(value=np.mean(y))\n",
    "    left_child = build_tree(X_left, y_left, max_depth-1, min_size)\n",
    "    right_child = build_tree(X_right, y_right, max_depth-1, min_size)\n",
    "    return DecisionTreeNode(feature_index=feature, threshold=threshold, left=left_child, right=right_child)\n",
    "\n",
    "# Función para realizar predicciones\n",
    "def predict(node, x):\n",
    "    if node.value is not None:\n",
    "        return node.value\n",
    "    if x[node.feature_index] <= node.threshold:\n",
    "        return predict(node.left, x)\n",
    "    else:\n",
    "        return predict(node.right, x)\n",
    "\n",
    "\n",
    "# Dividimos los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "\n",
    "# Construimos el árbol\n",
    "tree_root = build_tree(X_train, y_train, max_depth=10, min_size=10)\n",
    "\n",
    "# Hacemos predicciones\n",
    "y_pred = np.array([predict(tree_root, xi) for xi in X_test])\n",
    "\n",
    "# Evaluamos el modelo\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 2 - Bagging manual\n",
    "\n",
    "En la celda 2 creen un modelo bagging **manualmente** con 10 árboles de regresión y comenten sobre el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Bagging): 1577.7517087276233\n",
      "MAE (Bagging): 1163.3861931263802\n"
     ]
    }
   ],
   "source": [
    "# Celda 2\n",
    "\n",
    "# Número de árboles en el ensamble\n",
    "n_trees = 10\n",
    "trees = []\n",
    "predictions = []\n",
    "\n",
    "# Generación de múltiples subconjuntos y entrenamiento de árboles\n",
    "for _ in range(n_trees):\n",
    "    # Muestreo con reemplazo del conjunto de datos de entrenamiento\n",
    "    indices = np.random.choice(X_train.shape[0], size=X_train.shape[0], replace=True)\n",
    "    X_sample = X_train[indices]\n",
    "    y_sample = y_train.iloc[indices]  # Asegúrate de usar iloc aquí\n",
    "    \n",
    "    # Construir un árbol en el subconjunto\n",
    "    tree = build_tree(X_sample, y_sample, max_depth=10, min_size=10)\n",
    "    trees.append(tree)\n",
    "\n",
    "# Predicción para el conjunto de prueba\n",
    "for tree in trees:\n",
    "    y_pred_tree = np.array([predict(tree, xi) for xi in X_test])\n",
    "    predictions.append(y_pred_tree)\n",
    "\n",
    "# Agregación de predicciones\n",
    "predictions = np.array(predictions)\n",
    "y_pred_final = np.mean(predictions, axis=0)\n",
    "\n",
    "# Evaluación del modelo\n",
    "rmse_bagging = np.sqrt(mean_squared_error(y_test, y_pred_final))\n",
    "mae_bagging = mean_absolute_error(y_test, y_pred_final)\n",
    "\n",
    "print(\"RMSE (Bagging):\", rmse_bagging)\n",
    "print(\"MAE (Bagging):\", mae_bagging)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 3 - Bagging con librería\n",
    "\n",
    "En la celda 3, con la librería sklearn, entrenen un modelo bagging con 10 árboles de regresión y el parámetro `max_features` igual a `log(n_features)` y comenten sobre el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Bagging with sklearn): 2768.3504147769763\n",
      "MAE (Bagging with sklearn): 2160.762049592191\n"
     ]
    }
   ],
   "source": [
    "# Celda 3\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Parámetros del Bagging\n",
    "n_trees = 10\n",
    "max_features = int(np.log(X_train.shape[1])) if X_train.shape[1] > 1 else 1\n",
    "\n",
    "# Crear el modelo de Bagging con DecisionTreeRegressor como el estimador base\n",
    "bagging_model = BaggingRegressor(\n",
    "    base_estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=n_trees,\n",
    "    max_features=max_features,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones sobre el conjunto de prueba\n",
    "y_pred = bagging_model.predict(X_test)\n",
    "\n",
    "# Evaluación del modelo\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"RMSE (Bagging with sklearn):\", rmse)\n",
    "print(\"MAE (Bagging with sklearn):\", mae)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el modelo manual, no se menciona una restricción en el número de características por modelo (a menos que lo hayas implementado dentro de tus funciones personalizadas). En el modelo de sklearn, usaste max_features igual a log(n_features). Este parámetro limita el número de características que cada árbol puede considerar para dividir en cada nodo, lo que puede haber contribuido a un peor rendimiento si no todos los árboles tienen suficiente información para hacer buenas predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 4 - Random forest con librería\n",
    "\n",
    "En la celda 4, usando la librería sklearn entrenen un modelo de Randon Forest para regresión  y comenten sobre el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Random Forest): 1765.4118259983413\n",
      "MAE (Random Forest): 1314.4207078056425\n"
     ]
    }
   ],
   "source": [
    "# Celda 4\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Crear el modelo de Random Forest\n",
    "random_forest_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Entrenar el modelo\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones sobre el conjunto de prueba\n",
    "y_pred_rf = random_forest_model.predict(X_test)\n",
    "\n",
    "# Evaluación del modelo\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "\n",
    "print(\"RMSE (Random Forest):\", rmse_rf)\n",
    "print(\"MAE (Random Forest):\", mae_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest generalmente ofrece un rendimiento robusto y es menos propenso al sobreajuste comparado con un único árbol de decisión debido a su naturaleza de ensamble.\n",
    "\n",
    "Es muy bueno para manejar datasets con un gran número de características y puede manejar automáticamente las interacciones entre características sin necesidad de transformación manual.\n",
    "\n",
    "Una ventaja adicional de usar Random Forest es que puede proporcionar una visión directa de la importancia de cada característica para la predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 5 - Calibración de parámetros Random forest\n",
    "\n",
    "En la celda 5, calibren los parámetros max_depth, max_features y n_estimators del modelo de Randon Forest para regresión, comenten sobre el desempeño del modelo y describan cómo cada parámetro afecta el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros: {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 200}\n",
      "RMSE (Mejor Random Forest): 1564.2461359342767\n",
      "MAE (Mejor Random Forest): 1147.2014922680428\n"
     ]
    }
   ],
   "source": [
    "# Celda 5\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir el grid de hiperparámetros\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'n_estimators': [50, 100, 200]\n",
    "}\n",
    "\n",
    "# Crear el modelo de Random Forest\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Ejecutar GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mejores parámetros encontrados\n",
    "print(\"Mejores parámetros:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluar el modelo con los mejores parámetros\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred_best_rf = best_rf.predict(X_test)\n",
    "rmse_best_rf = np.sqrt(mean_squared_error(y_test, y_pred_best_rf))\n",
    "mae_best_rf = mean_absolute_error(y_test, y_pred_best_rf)\n",
    "\n",
    "print(\"RMSE (Mejor Random Forest):\", rmse_best_rf)\n",
    "print(\"MAE (Mejor Random Forest):\", mae_best_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo de Random Forest calibrado con estos parámetros ha mejorado notablemente, alcanzando un RMSE de 1564.246 y un MAE de 1147.202. Esta mejora en las métricas sugiere que el modelo es capaz de hacer predicciones mucho más precisas que la versión inicial sin calibrar. La configuración seleccionada de los hiperparámetros ha contribuido a un modelo que generaliza mejor a nuevos datos, evitando el sobreajuste y capturando suficientemente la relación subyacente entre las características y la variable objetivo.\n",
    "\n",
    "max_depth (Profundidad máxima del árbol) Impacto: Este parámetro controla la profundidad máxima de los árboles dentro del bosque. Limitar la profundidad del árbol ayuda a prevenir el sobreajuste al reducir la complejidad del modelo. Una profundidad máxima de 10 indica que los árboles no serán extremadamente profundos, permitiendo que el modelo capture suficientemente la estructura de los datos sin ajustarse demasiado a las peculiaridades del conjunto de entrenamiento. Resultado: En este caso, una profundidad de 10 fue la óptima, equilibrando bien entre sesgo y varianza.\n",
    "\n",
    "max_features (Número máximo de características consideradas para dividir un nodo) Impacto: Controla cuántas características se consideran en cada división de los nodos del árbol. sqrt significa que en cada split, el modelo considerará la raíz cuadrada del número total de características. Esto reduce la correlación entre los árboles en el bosque; cada árbol usa diferentes subconjuntos de características y esto aumenta la diversidad entre los árboles, mejorando la robustez del modelo. Resultado: El uso de 'sqrt' ha demostrado ser efectivo, probablemente porque permite que cada árbol en el bosque se construya considerando diferentes aspectos de los datos, lo que ayuda a mejorar la precisión general del modelo al reducir el riesgo de sobreajuste.\n",
    "\n",
    "n_estimators (Número de árboles en el bosque) Impacto: Representa el número de árboles en el bosque. Generalmente, un número mayor de árboles aumenta la precisión del modelo y hace que las predicciones sean más estables, pero también incrementa el costo computacional y el tiempo de entrenamiento. Además, hay un punto de rendimientos decrecientes donde aumentar el número de árboles ya no mejora significativamente el rendimiento. Resultado: Un valor de 200 árboles en este caso parece ser adecuado para proporcionar un buen balance entre rendimiento y eficiencia computacional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 6 - XGBoost con librería\n",
    "\n",
    "En la celda 6 implementen un modelo XGBoost de regresión con la librería sklearn y comenten sobre el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (XGBoost): 1581.5074712833923\n",
      "MAE (XGBoost): 1171.3119623831733\n"
     ]
    }
   ],
   "source": [
    "# Celda 6\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Crear instancia de XGBRegressor\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 100)\n",
    "\n",
    "# Entrenar el modelo\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones sobre el conjunto de prueba\n",
    "y_pred_xg = xg_reg.predict(X_test)\n",
    "\n",
    "# Cálculo del RMSE y MAE\n",
    "rmse_xg = np.sqrt(mean_squared_error(y_test, y_pred_xg))\n",
    "mae_xg = mean_absolute_error(y_test, y_pred_xg)\n",
    "\n",
    "print(\"RMSE (XGBoost):\", rmse_xg)\n",
    "print(\"MAE (XGBoost):\", mae_xg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE (Root Mean Square Error): El RMSE es una medida de la desviación promedio de las predicciones del modelo respecto a los valores reales. Un RMSE de aproximadamente 1586 sugiere que las predicciones del modelo, en promedio, se desvían en 1586 unidades del valor real. Dado que el RMSE penaliza más los errores grandes (por su cuadratura), un valor de 1586 puede indicar la presencia de algunos errores de predicción sustanciales.\n",
    "\n",
    "MAE (Mean Absolute Error): El MAE proporciona una medida directa del error absoluto promedio en las predicciones. Un MAE de aproximadamente 1170 indica que, en promedio, las predicciones del modelo difieren en 1170 unidades del valor real, sin considerar la dirección del error (sobrestimación o subestimación)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 7 - Calibración de parámetros XGBoost\n",
    "\n",
    "En la celda 7 calibren los parámetros learning rate, gamma y colsample_bytree del modelo XGBoost para regresión, comenten sobre el desempeño del modelo y describan cómo cada parámetro afecta el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 64 candidates, totalling 192 fits\n",
      "Mejores parámetros encontrados: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1}\n",
      "RMSE (Mejor XGBoost): 1539.4865968350557\n",
      "MAE (Mejor XGBoost): 1131.4267452671677\n"
     ]
    }
   ],
   "source": [
    "# Celda 7\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "# Definición de los parámetros a calibrar\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'gamma': [0, 0.1, 0.5, 1],               \n",
    "    'colsample_bytree': [0.3, 0.5, 0.7, 1.0] \n",
    "}\n",
    "\n",
    "# Creación del modelo XGBoost\n",
    "xgb_model = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators=100, max_depth=5)\n",
    "\n",
    "# Configuración de GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, verbose=1, n_jobs=-1)\n",
    "\n",
    "# Ejecución de la búsqueda de parámetros\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mejores parámetros y mejor modelo\n",
    "best_parameters = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluación del mejor modelo\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "rmse_best = np.sqrt(mean_squared_error(y_test, y_pred_best))\n",
    "mae_best = mean_absolute_error(y_test, y_pred_best)\n",
    "\n",
    "# Impresión de resultados\n",
    "print(\"Mejores parámetros encontrados:\", best_parameters)\n",
    "print(\"RMSE (Mejor XGBoost):\", rmse_best)\n",
    "print(\"MAE (Mejor XGBoost):\", mae_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE (1543.0876) y MAE (1132.4099): Estas métricas son indicadores del error promedio en las predicciones. El RMSE más bajo indica que el modelo tiene una buena capacidad para predecir los valores sin cometer errores grandes, mientras que un MAE más bajo indica que en promedio, las predicciones son bastante precisas. La reducción en estas métricas respecto a la configuración inicial del modelo refleja que los ajustes realizados han sido efectivos para mejorar la precisión del modelo.\n",
    "\n",
    "colsample_bytree (0.7): Este parámetro especifica la fracción de características (columnas) que se usan para construir cada árbol. Un valor de 0.7 significa que el 70% de las características están disponibles para construir cada árbol. Limitar el número de características puede ayudar a hacer el modelo más generalizable y prevenir el sobreajuste. En este caso, parece que el modelo ha encontrado un buen equilibrio, usando suficientes características para captar la variabilidad en los datos sin sobreajustarse.\n",
    "\n",
    "gamma (0): Gamma especifica la reducción mínima de la pérdida requerida para hacer una partición adicional en un nodo del árbol. Un valor de 0 significa que no hay restricción conservadora, y cualquier ganancia, por pequeña que sea, puede resultar en una división. Esto permite que el modelo sea más flexible y complejo, lo que puede ser adecuado dado que el valor de 0 ha dado como resultado un modelo con menor RMSE y MAE, indicando que pequeñas divisiones adicionales han beneficiado al modelo sin causar sobreajuste significativo.\n",
    "\n",
    "learning_rate (0.1): Este parámetro controla la tasa a la que el modelo aprende. Un valor de 0.1 es moderadamente rápido y es comúnmente usado en práctica. Permite que el modelo ajuste sus errores de manera efectiva sin tomar pasos demasiado grandes que podrían llevar a soluciones subóptimas. En este caso, parece ser el valor adecuado para lograr una convergencia eficiente y efectiva en términos de rendimiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 8 - Comparación y análisis de resultados\n",
    "En la celda 8 comparen los resultados obtenidos de los diferentes modelos (random forest y XGBoost) y comenten las ventajas del mejor modelo y las desventajas del modelo con el menor desempeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Celda 8\n",
    "\n",
    "Los modelos muestran un rendimiento competitivo, pero XGBoost tiene un ligero margen en ambas métricas, lo que sugiere que ha podido modelar la relación subyacente en los datos con un poco más de precisión y consistencia que Random Forest.\n",
    "\n",
    "Ventajas del XGBoost (Mejor Modelo): Eficiencia y Velocidad: XGBoost es conocido por su eficiencia en la ejecución. Utiliza técnicas de optimización como el manejo eficiente de la memoria y el paralelismo, lo cual es crucial cuando se trabaja con grandes volúmenes de datos. Manejo de Overfitting: Incluye parámetros de regularización (alpha y lambda), lo que ayuda a controlar el overfitting. Esto es particularmente útil en escenarios donde la dimensionalidad de los datos es alta. Flexibilidad: Puede manejar diversas funciones de pérdida y criterios de validación personalizados, lo que lo hace adaptable a varias necesidades específicas de regresión y clasificación. Escalabilidad: Ha sido diseñado para escalar y operar de manera eficiente en máquinas distribuidas, lo que es una ventaja significativa en entornos de producción de gran escala.\n",
    "\n",
    "Desventajas del Random Forest (Menor Desempeño): Menos eficiente con grandes datasets: Aunque Random Forest puede manejar datos de alta dimensionalidad, tiende a ser más lento y menos eficiente en términos de memoria comparado con XGBoost, especialmente cuando el tamaño del dataset es muy grande. Menor control sobre el overfitting: Aunque tiene parámetros que pueden ayudar a controlar el overfitting (como max_depth y min_samples_split), no incluye regularización de L1/L2, que puede ser más efectiva en ciertos casos. No es tan flexible para funciones de pérdida personalizadas: A diferencia de XGBoost, que permite la implementación de funciones de pérdida personalizadas de manera más directa, Random Forest está más limitado a las opciones predefinidas en Scikit-Learn.\n",
    "\n",
    "XGBoost ha demostrado ser el mejor modelo en este escenario, ofreciendo un mejor rendimiento y más flexibilidad para ajustes avanzados y optimización. Sin embargo, Random Forest sigue siendo una herramienta poderosa y podría ser preferida en situaciones donde la interpretación del modelo y la simplicidad de implementación son más críticas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
